{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Importing Required Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T06:47:29.580587Z",
     "iopub.status.busy": "2023-03-20T06:47:29.580177Z",
     "iopub.status.idle": "2023-03-20T06:47:38.943791Z",
     "shell.execute_reply": "2023-03-20T06:47:38.942500Z",
     "shell.execute_reply.started": "2023-03-20T06:47:29.580549Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AdamW, AlbertForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparing the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T06:47:38.967218Z",
     "iopub.status.busy": "2023-03-20T06:47:38.964483Z",
     "iopub.status.idle": "2023-03-20T06:47:51.953120Z",
     "shell.execute_reply": "2023-03-20T06:47:51.952041Z",
     "shell.execute_reply.started": "2023-03-20T06:47:38.967154Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-xlarge-v2\")\n",
    "model = AlbertForSequenceClassification.from_pretrained(\"albert-xlarge-v2\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device) # Moving the model to GPU if using one\n",
    "\n",
    "num_of_sentences_to_order = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Converting Raw Data to Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T06:47:51.957066Z",
     "iopub.status.busy": "2023-03-20T06:47:51.956771Z",
     "iopub.status.idle": "2023-03-20T06:47:51.977256Z",
     "shell.execute_reply": "2023-03-20T06:47:51.976163Z",
     "shell.execute_reply.started": "2023-03-20T06:47:51.957039Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_pickle_data(addr, mode=\"rb\"):\n",
    "    with open(addr, mode) as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "def write_pickle_data(addr, data, mode=\"wb\"):\n",
    "    with open(addr, mode) as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "def create_pairs_from_orig_data(orig_data):\n",
    "    pairs = []\n",
    "\n",
    "    for key in orig_data: # Labelling 0 if the second sentence is the next sentence of the first one, and 1 otherwise\n",
    "        example = orig_data[key]\n",
    "        pair = sorted(zip(example[1], example[0]))\n",
    "        # Looping over all permutations of selecting two sentences from all sentences in a set\n",
    "        for i, j in itertools.permutations(range(num_of_sentences_to_order), 2):\n",
    "            if j == i + 1: # As 0s are much less than 1s, adding 0s twice can make the dataset more balanced\n",
    "                pairs.append([pair[i][1].lower(), pair[j][1].lower(), 0])\n",
    "                pairs.append([pair[i][1].lower(), pair[j][1].lower(), 0])\n",
    "            else:\n",
    "                pairs.append([pair[i][1].lower(), pair[j][1].lower(), 1])\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def create_dataset_from_pairs(pairs):\n",
    "    input_ids, token_type_ids, attention_mask, labels = [], [], [], []\n",
    "\n",
    "    for pair in pairs:\n",
    "        sentence1, sentence2, label = pair\n",
    "        encoding = tokenizer.encode_plus(sentence1, sentence2,\n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=128,\n",
    "                                         padding=\"max_length\",\n",
    "                                         return_tensors=\"pt\")\n",
    "\n",
    "        input_ids.append(encoding[\"input_ids\"])\n",
    "        token_type_ids.append(encoding[\"token_type_ids\"])\n",
    "        attention_mask.append(encoding[\"attention_mask\"])\n",
    "        labels.append(label)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "    attention_mask = torch.cat(attention_mask, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    ds = TensorDataset(input_ids, token_type_ids, attention_mask, labels)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def get_train_val_datasets(ds, val_size=512):\n",
    "    train_ds, val_ds = random_split(ds, [len(ds) - val_size, val_size])\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def get_datasets():\n",
    "    train_data_orig = read_pickle_data(\"data/train.pickle\")\n",
    "    pairs = create_pairs_from_orig_data(train_data_orig)\n",
    "    ds = create_dataset_from_pairs(pairs)\n",
    "    # write_pickle_data(\"data/ds.pickle\", ds)\n",
    "    # ds = read_pickle_data(\"data/ds.pickle\")\n",
    "    train_ds, val_ds = get_train_val_datasets(ds)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Getting Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T06:47:51.979367Z",
     "iopub.status.busy": "2023-03-20T06:47:51.978960Z",
     "iopub.status.idle": "2023-03-20T06:51:24.789554Z",
     "shell.execute_reply": "2023-03-20T06:51:24.788422Z",
     "shell.execute_reply.started": "2023-03-20T06:47:51.979314Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configuring Train and Validation Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T06:51:24.792114Z",
     "iopub.status.busy": "2023-03-20T06:51:24.791095Z",
     "iopub.status.idle": "2023-03-20T06:51:24.798393Z",
     "shell.execute_reply": "2023-03-20T06:51:24.797226Z",
     "shell.execute_reply.started": "2023-03-20T06:51:24.792074Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_samples = 17000\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, sampler=RandomSampler(train_ds, num_samples=num_samples), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_ds, sampler=SequentialSampler(val_ds), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configuring Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T18:54:36.387749Z",
     "iopub.status.busy": "2023-03-20T18:54:36.387216Z",
     "iopub.status.idle": "2023-03-20T18:54:36.463355Z",
     "shell.execute_reply": "2023-03-20T18:54:36.461579Z",
     "shell.execute_reply.started": "2023-03-20T18:54:36.387717Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "learning_rate = 8e-6\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, no_deprecation_warning=True)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
    "best_val_loss = math.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Checking Dataloaders Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T17:14:10.565281Z",
     "iopub.status.busy": "2023-03-20T17:14:10.564708Z",
     "iopub.status.idle": "2023-03-20T17:14:10.572282Z",
     "shell.execute_reply": "2023-03-20T17:14:10.570870Z",
     "shell.execute_reply.started": "2023-03-20T17:14:10.565243Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T17:14:23.817193Z",
     "iopub.status.busy": "2023-03-20T17:14:23.816825Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "accum_num = 4 # Number of Epochs to Accumulate Gradients\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    # Train Phase\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_token_type_ids = batch[1].to(device)\n",
    "        batch_attention_mask = batch[2].to(device)\n",
    "        batch_labels = batch[3].to(device)\n",
    "\n",
    "        res = model(batch_input_ids,\n",
    "                    token_type_ids=batch_token_type_ids,\n",
    "                    attention_mask=batch_attention_mask,\n",
    "                    labels=batch_labels,\n",
    "                    return_dict=True)\n",
    "\n",
    "        batch_loss = res.loss\n",
    "        train_loss += batch_loss.item()\n",
    "        batch_loss.backward()\n",
    "\n",
    "        if ((step + 1) % accum_num == 0) or (step + 1 == len(train_dataloader)):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(f\"Step {step} Passed.\")\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Evaluation Phase\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for batch in val_dataloader:\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_token_type_ids = batch[1].to(device)\n",
    "        batch_attention_mask = batch[2].to(device)\n",
    "        batch_labels = batch[3].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            res = model(batch_input_ids,\n",
    "                        token_type_ids=batch_token_type_ids,\n",
    "                        attention_mask=batch_attention_mask,\n",
    "                        labels=batch_labels,\n",
    "                        return_dict=True)\n",
    "\n",
    "            batch_loss = res.loss\n",
    "\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch Validation Loss: {avg_val_loss}\")\n",
    "    print(f\"Epoch Train Loss: {avg_train_loss}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss: # Checkpoint Model with the Least Validation Loss Value\n",
    "        best_val_loss = avg_val_loss\n",
    "        model.save_pretrained(\"models/best_model\")\n",
    "\n",
    "# Plotting Train and Validation Loss\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T16:33:48.856721Z",
     "iopub.status.busy": "2023-03-20T16:33:48.854157Z",
     "iopub.status.idle": "2023-03-20T16:33:48.892831Z",
     "shell.execute_reply": "2023-03-20T16:33:48.891880Z",
     "shell.execute_reply.started": "2023-03-20T16:33:48.856683Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data_orig = read_pickle_data(\"data/test.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reordering Test Data and Generating the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-03-20T16:33:48.899704Z",
     "iopub.status.busy": "2023-03-20T16:33:48.897265Z",
     "iopub.status.idle": "2023-03-20T17:12:58.162445Z",
     "shell.execute_reply": "2023-03-20T17:12:58.161371Z",
     "shell.execute_reply.started": "2023-03-20T16:33:48.899666Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result_rows = []\n",
    "model.eval()\n",
    "for index in test_data_orig:\n",
    "    # Creating an N x N matrix. Matrix[i][j] is the output of the model for the ith sentence followed by the jth sentence (class 0 in the classification scheme)\n",
    "    prob_mat = []\n",
    "    example = test_data_orig[index][0]\n",
    "    for i in range(num_of_sentences_to_order):\n",
    "        row = []\n",
    "        for j in range(num_of_sentences_to_order):\n",
    "            if i == j:\n",
    "                row.append(0)\n",
    "                continue\n",
    "            else:\n",
    "                encoding = tokenizer.encode_plus(example[i], example[j],\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 max_length=128,\n",
    "                                                 padding=\"max_length\",\n",
    "                                                 return_tensors=\"pt\")\n",
    "                input_ids = encoding[\"input_ids\"].to(device)\n",
    "                token_type_ids = encoding[\"token_type_ids\"].to(device)\n",
    "                attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    res = model(input_ids,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                return_dict=True)\n",
    "\n",
    "                    row.append(res.logits.detach().cpu().numpy()[0][0]) # 0 is the class that the second sequence comes after the first one\n",
    "\n",
    "        prob_mat.append(row)\n",
    "\n",
    "    # Finding the permutation with the highest sum of output model for each pair of consecutive sentences in it\n",
    "    max_prob = -math.inf\n",
    "    max_prob_permutation = None\n",
    "    for permutation in itertools.permutations(range(num_of_sentences_to_order)):\n",
    "        prob_sum = 0\n",
    "        for i in range(num_of_sentences_to_order - 1):\n",
    "            prob_sum += prob_mat[permutation[i]][permutation[i+1]]\n",
    "\n",
    "        if prob_sum > max_prob:\n",
    "            max_prob = prob_sum\n",
    "            max_prob_permutation = permutation\n",
    "\n",
    "    # Putting indexes in order\n",
    "    result_row = [index]\n",
    "    for i in range(num_of_sentences_to_order):\n",
    "        result_row.append(max_prob_permutation.index(i))\n",
    "\n",
    "    result_rows.append(result_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Writing Results to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [\"id\"]\n",
    "for i in range(len(num_of_sentences_to_order)):\n",
    "    columns.append(f\"index_{i+1}\")\n",
    "pd.DataFrame(result_rows, columns=columns).to_csv(\"results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
